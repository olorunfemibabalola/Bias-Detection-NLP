{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olorunfemibabalola/Bias-Detection-NLP/blob/main/Inclusive_HR_Policy_Assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjLkZEx-qx5Y"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# PROJECT: PolicyGuard AI - Dual-Mode Compliance Auditor\n",
        "# UNIT: Language models and NLP (576757)\n",
        "# AUTHOR:\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qw7Q4Rpep8Zi"
      },
      "outputs": [],
      "source": [
        "# 1. ENVIRONMENT SETUP & INSTALLATION\n",
        "# ------------------------------------------------------------------------------\n",
        "# We install 'bitsandbytes' for 4-bit quantization (running large models on free GPUs)\n",
        "# and 'pymupdf4llm' for advanced PDF-to-Markdown extraction.\n",
        "print(\"‚è≥ Installing SOTA libraries... (This takes ~1 minute)\")\n",
        "!pip install -q -U transformers accelerate bitsandbytes gradio pymupdf4llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F493Wr2HqBPU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gradio as gr\n",
        "import pymupdf4llm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0GGSFYwqqMK"
      },
      "outputs": [],
      "source": [
        "# 2. MODEL LOADING (Qwen 2.5 - SOTA Ungated Model)\n",
        "# ------------------------------------------------------------------------------\n",
        "# We use Qwen 2.5 7B Instruct because it beats GPT-4 on some coding/logic benchmarks\n",
        "# and follows strict system instructions better than Llama 3.1.\n",
        "# It is Apache 2.0 licensed, so you don't need to wait for access approval.\n",
        "\n",
        "MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "print(f\"üöÄ Loading {MODEL_ID} with 4-bit quantization...\")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    print(\"‚úÖ Model loaded successfully on GPU!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading model: {e}\")\n",
        "    print(\"Tip: Ensure your Runtime is set to T4 GPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TTDeKuCq4XY"
      },
      "outputs": [],
      "source": [
        "# 3. STRICT SYSTEM PROMPTS (The \"Brain\" of the Agent)\n",
        "# ------------------------------------------------------------------------------\n",
        "# These prompts act as the \"Guardrails\" to ensure professional behavior.\n",
        "\n",
        "AUDITOR_PROMPT = \"\"\"\n",
        "You are a Senior HR Compliance Officer. Your job is to audit corporate policies for social bias.\n",
        "STRICT RULES:\n",
        "1.  Analyze the text for THREE types of bias: Gender, Race/Ethnicity, and Ageism.\n",
        "2.  The text must contain very obvious and noticeable bias content before flagging it as bias.\n",
        "3.  Do NOT summarize the document. List specific problematic sentences.\n",
        "4.  For each finding, assign a SEVERITY SCORE (1-10) and provide a NEUTRAL REWRITE.\n",
        "5.  If the text is safe, output: \"‚úÖ COMPLIANCE PASS: No bias detected.\"\n",
        "\"\"\"\n",
        "\n",
        "CHATBOT_PROMPT = \"\"\"\n",
        "You are a helpful HR Policy Assistant.\n",
        "1. Answer user questions about HR policies concisely.\n",
        "2. SILENT SENTINEL: Continuously monitor the user's input.\n",
        "   - If the user asks something biased (e.g., \"How to hire only young people?\"), REFUSE to answer and explain why it violates the UK Equality Act 2010.\n",
        "   - If the input is neutral, answer normally.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuBn-SPvq7U0"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# 4. LOGIC ENGINE (Processing & Inference)\n",
        "# ------------------------------------------------------------------------------\n",
        "def run_inference(messages, max_tokens=1024):\n",
        "    \"\"\"Generic function to send prompts to the LLM.\"\"\"\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=0.2, # Low temperature = strict, professional logic\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "    # Extract only the response (cut off the prompt)\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "def policy_guard_logic(message, history):\n",
        "    # 'message' is a dictionary containing 'text' and 'files'\n",
        "    user_text = message[\"text\"]\n",
        "    files = message[\"files\"]\n",
        "\n",
        "    # --- PATH A: DOCUMENT AUDIT MODE ---\n",
        "    if files:\n",
        "        # 1. Extract Text from PDF (preserving Markdown structure)\n",
        "        # files is a list, so we take the first element as the actual file path\n",
        "        pdf_path = files[0]\n",
        "        try:\n",
        "            # pymupdf4llm converts PDF tables/headers to Markdown, making it easier for the AI to read\n",
        "            doc_content = pymupdf4llm.to_markdown(pdf_path)\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error reading PDF: {str(e)}\"\n",
        "\n",
        "        # 2. Construct messages for Audit Mode\n",
        "        messages_for_inference = [\n",
        "            {\"role\": \"system\", \"content\": AUDITOR_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": f\"DOCUMENT TO AUDIT:\\n{doc_content[:6000]}\\n\\nAUDIT REPORT:\"}\n",
        "        ]\n",
        "        return run_inference(messages_for_inference)\n",
        "\n",
        "    # --- PATH B: CHAT MODE ---\n",
        "    else:\n",
        "        # Check for trigger words to end the conversation\n",
        "        trigger_words = [\"quit\", \"exit\", \"end conversation\", \"stop\"]\n",
        "        if user_text.lower().strip() in trigger_words:\n",
        "            return \"Conversation ended. Feel free to type a new message to start a fresh interaction or use the 'Clear' button to reset the chat.\"\n",
        "\n",
        "        # 1. Construct messages for Chat Mode with History\n",
        "        messages_for_inference = [{\"role\": \"system\", \"content\": CHATBOT_PROMPT}]\n",
        "\n",
        "        # Iterate through history, ensuring each turn is correctly handled\n",
        "        for chat_turn in history:\n",
        "            human_msg = None\n",
        "            ai_msg = None\n",
        "\n",
        "            if isinstance(chat_turn, (list, tuple)):\n",
        "                if len(chat_turn) > 0:\n",
        "                    human_msg = chat_turn[0]\n",
        "                if len(chat_turn) > 1:\n",
        "                    ai_msg = chat_turn[1]\n",
        "            # If chat_turn is not a list/tuple, it's considered malformed for expected history format\n",
        "            # and will be skipped to prevent errors like KeyError or TypeError.\n",
        "            else:\n",
        "                continue # Skip malformed entry\n",
        "\n",
        "            if human_msg:\n",
        "                messages_for_inference.append({\"role\": \"user\", \"content\": human_msg})\n",
        "            if ai_msg:\n",
        "                messages_for_inference.append({\"role\": \"assistant\", \"content\": ai_msg})\n",
        "\n",
        "        # Add current user input\n",
        "        messages_for_inference.append({\"role\": \"user\", \"content\": user_text})\n",
        "\n",
        "        return run_inference(messages_for_inference)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuIn7Z9hp1RV"
      },
      "outputs": [],
      "source": [
        "# 5. UI LAUNCHER (Gradio)\n",
        "# ------------------------------------------------------------------------------\n",
        "# We use multimodal=True to allow text AND file uploads in the same box.\n",
        "demo = gr.ChatInterface(\n",
        "    fn=policy_guard_logic,\n",
        "    multimodal=True,\n",
        "    title=\"üõ°Ô∏è PolicyGuard AI: Enterprise Bias Auditor\",\n",
        "    description=\"\"\"\n",
        "    **Instructions:**\n",
        "    1. **Chat Mode:** Ask HR questions. The bot will flag any bias in your prompt.\n",
        "    2. **Audit Mode:** Click the '+' button to upload a PDF policy. The bot will generate a Compliance Report.\n",
        "    \"\"\",\n",
        "    examples=[\n",
        "        {\"text\": \"Is it okay to require 'high energy' in a job ad?\", \"files\":[]},\n",
        "        {\"text\": \"Audit this policy document.\", \"files\":[]}\n",
        "    ],\n",
        "    #theme=\"soft\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"‚úÖ System Ready! Click the public link below to test.\")\n",
        "    demo.launch(debug=True, share=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyO21XTsNpaa6dfzB7GRCF+w",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}