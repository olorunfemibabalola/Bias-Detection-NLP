{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olorunfemibabalola/Bias-Detection-NLP/blob/main/Inclusive_HR_Policy_Assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PROJECT:** Inclusive HR Policy Assistant & Auditor\n",
        "\n",
        "**UNIT:** Language models and NLP (576757)\n",
        "\n",
        "**AUTHOR:** Babalola Praise Olorunfemi\n",
        "\n",
        "**STUDENT ID:** s5819556\n"
      ],
      "metadata": {
        "id": "s6euei0rJCNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "==============================================================================\n"
      ],
      "metadata": {
        "id": "bX8NpdVTJ8Po"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qw7Q4Rpep8Zi"
      },
      "outputs": [],
      "source": [
        "# 1. ENVIRONMENT SETUP & INSTALLATION\n",
        "# ------------------------------------------------------------------------------\n",
        "# First, we install some key libraries:\n",
        "# 'bitsandbytes' helps run big models on smaller GPUs.\n",
        "# 'pymupdf4llm' extracts text/tables from PDFs into Markdown for our AI.\n",
        "# 'transformers' is for working with AI models.\n",
        "# 'accelerate' speeds up model training/inference.\n",
        "# 'gradio' builds our web interface.\n",
        "print(\"‚è≥ Installing SOTA libraries... (This takes ~1 minute)\")\n",
        "!pip install -q -U transformers accelerate bitsandbytes gradio pymupdf4llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F493Wr2HqBPU"
      },
      "outputs": [],
      "source": [
        "# Importing the tools we'll need:\n",
        "import torch # For deep learning and GPU stuff.\n",
        "import gradio as gr # To make our user interface (UI).\n",
        "import pymupdf4llm # To read PDFs for the AI.\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig # From Hugging Face:\n",
        "# AutoModelForCausalLM: Loads AI text-generation models.\n",
        "# AutoTokenizer: Converts text to numbers for the model.\n",
        "# BitsAndBytesConfig: Helps make models smaller to save memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0GGSFYwqqMK"
      },
      "outputs": [],
      "source": [
        "# 2. MODEL LOADING (Qwen 2.5 - SOTA Ungated Model)\n",
        "# ------------------------------------------------------------------------------\n",
        "# Loading the AI's brain: the large language model!\n",
        "# We're using \"Qwen 2.5 7B Instruct\" because it's good at following instructions and is free to use.\n",
        "\n",
        "MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\" # The specific model name.\n",
        "\n",
        "print(f\"üöÄ Loading {MODEL_ID} with 4-bit quantization...\") # Progress message.\n",
        "\n",
        "# Configuration to load the model using less memory (4-bit quantization).\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "try:\n",
        "    # Loading the tokenizer (to understand text) and the model itself (the AI).\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        quantization_config=bnb_config, # Apply our memory-saving settings.\n",
        "        device_map=\"auto\" # Automatically uses the GPU if available.\n",
        "    )\n",
        "    print(\"‚úÖ Model loaded successfully on GPU!\") # Success!\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading model: {e}\") # Uh oh, something went wrong!\n",
        "    print(\"Tip: Ensure your Runtime is set to T4 GPU.\") # Hint for common issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TTDeKuCq4XY"
      },
      "outputs": [],
      "source": [
        "# 3. STRICT SYSTEM PROMPTS (The \"Brain\" of the Agent)\n",
        "# ------------------------------------------------------------------------------\n",
        "# These are the important rules that tell our AI how to act.\n",
        "\n",
        "# Rules for 'Auditor' mode: checking documents for bias.\n",
        "AUDITOR_PROMPT = \"\"\"\n",
        "You are a Senior HR Compliance Officer. Your job is to audit corporate policies for social bias.\n",
        "STRICT RULES:\n",
        "1.  Analyze the text for THREE types of bias: Gender, Race/Ethnicity, and Ageism.\n",
        "2.  The text must contain very obvious and noticeable bias content before flagging it as bias.\n",
        "3.  Do NOT summarize the document. List specific problematic sentences.\n",
        "4.  For each finding, assign a SEVERITY SCORE (1-10) and provide a NEUTRAL REWRITE.\n",
        "5.  If the text is safe, output: \"‚úÖ COMPLIANCE PASS: No bias detected.\"\n",
        "\"\"\"\n",
        "\n",
        "# Rules for 'Chatbot' mode: answering HR questions.\n",
        "CHATBOT_PROMPT = \"\"\"\n",
        "You are a helpful HR Policy Assistant.\n",
        "1. Answer user questions about HR policies concisely.\n",
        "2. SILENT SENTINEL: Continuously monitor the user's input.\n",
        "   - If the user asks something biased (e.g., \"How to hire only young people?\"), REFUSE to answer and explain why it violates the UK Equality Act 2010.\n",
        "   - If the input is neutral, answer normally.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuBn-SPvq7U0"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# 4. LOGIC ENGINE (Processing & Inference)\n",
        "# ------------------------------------------------------------------------------\n",
        "# This is the main part that makes the AI think and respond.\n",
        "\n",
        "def run_inference(messages, max_tokens=1024):\n",
        "    \"\"\"Sends questions to the AI model and gets its response.\"\"\"\n",
        "    # Formats our conversation messages so the AI understands them.\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    # Converts the text into numbers for the model and moves it to the GPU.\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Tells the model to generate an answer.\n",
        "    # 'max_new_tokens' limits length, 'temperature' makes it less random, 'top_p' controls creativity.\n",
        "    generated_ids = model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=0.2, # Keeps responses focused.\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "    # Removes the original prompt from the AI's output to get just the new response.\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    # Turns the numbers back into readable text.\n",
        "    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "def policy_guard_logic(message, history):\n",
        "    # This function runs every time a user types or uploads something.\n",
        "    user_text = message[\"text\"] # What the user typed.\n",
        "    files = message[\"files\"] # Any files they uploaded.\n",
        "\n",
        "    # --- PATH A: DOCUMENT AUDIT MODE ---\n",
        "    # If files are uploaded, we're auditing them.\n",
        "    if files:\n",
        "        # 1. Get text from PDF.\n",
        "        # 'files' is a list, so we take the first file path.\n",
        "        pdf_path = files[0]\n",
        "        try:\n",
        "            # Reads the PDF and turns it into Markdown for the AI.\n",
        "            doc_content = pymupdf4llm.to_markdown(pdf_path)\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error reading PDF: {str(e)}\"\n",
        "\n",
        "        # 2. Prepare messages for the Auditor AI.\n",
        "        # Includes the auditor rules and the document content (up to 6000 characters).\n",
        "        messages_for_inference = [\n",
        "            {\"role\": \"system\", \"content\": AUDITOR_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": f\"DOCUMENT TO AUDIT:\\n{doc_content[:6000]}\\n\\nAUDIT REPORT:\"}\n",
        "        ]\n",
        "        # Send to the AI and return its report.\n",
        "        return run_inference(messages_for_inference)\n",
        "\n",
        "    # --- PATH B: CHAT MODE ---\n",
        "    # If no files, it's just a normal chat.\n",
        "    else:\n",
        "        # Check if the user typed a word to end the conversation.\n",
        "        trigger_words = [\"quit\", \"exit\", \"end conversation\", \"stop\"]\n",
        "        if user_text.lower().strip() in trigger_words:\n",
        "            return \"Conversation ended. Feel free to type a new message to start a fresh interaction or use the 'Clear' button to reset the chat.\"\n",
        "\n",
        "        # 1. Build the chat history for the AI.\n",
        "        # Start with the chatbot's rules.\n",
        "        messages_for_inference = [{\"role\": \"system\", \"content\": CHATBOT_PROMPT}]\n",
        "\n",
        "        # Loop through past messages to add them to the AI's memory.\n",
        "        for chat_turn in history:\n",
        "            human_msg = None # User's message.\n",
        "            ai_msg = None # AI's reply.\n",
        "\n",
        "            # Make sure it's a list/tuple before trying to get messages.\n",
        "            if isinstance(chat_turn, (list, tuple)):\n",
        "                if len(chat_turn) > 0:\n",
        "                    human_msg = chat_turn[0]\n",
        "                if len(chat_turn) > 1:\n",
        "                    ai_msg = chat_turn[1]\n",
        "            else:\n",
        "                continue # Skip weird entries.\n",
        "\n",
        "            # Add valid past messages to the list.\n",
        "            if human_msg:\n",
        "                messages_for_inference.append({\"role\": \"user\", \"content\": human_msg})\n",
        "            if ai_msg:\n",
        "                messages_for_inference.append({\"role\": \"assistant\", \"content\": ai_msg})\n",
        "\n",
        "        # Add the user's *current* message.\n",
        "        messages_for_inference.append({\"role\": \"user\", \"content\": user_text})\n",
        "\n",
        "        # Send the whole conversation to the AI.\n",
        "        return run_inference(messages_for_inference)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuIn7Z9hp1RV"
      },
      "outputs": [],
      "source": [
        "# 5. UI LAUNCHER (Gradio)\n",
        "# ------------------------------------------------------------------------------\n",
        "# This part sets up our user interface (UI) for the app!\n",
        "# `multimodal=True` is cool because it lets us use text and file uploads in the same chat box.\n",
        "demo = gr.ChatInterface(\n",
        "    fn=policy_guard_logic, # This connects our main logic to the UI.\n",
        "    multimodal=True, # Allow text and files.\n",
        "    title=\"üõ°Ô∏è PolicyGuard AI: Enterprise Bias Auditor\", # Title for the app.\n",
        "    description=\"\"\"\n",
        "    **Instructions:** # How to use the app.\n",
        "    1. **Chat Mode:** Ask HR questions. Bot flags bias.\n",
        "    2. **Audit Mode:** Upload a PDF policy (using '+') for a compliance report.\n",
        "    \"\"\",\n",
        "    # Some example inputs to quickly test the app.\n",
        "    examples=[\n",
        "        {\"text\": \"Is it okay to require 'high energy' in a job ad?\", \"files\":[]}, # Chat example.\n",
        "        {\"text\": \"Audit this policy document.\", \"files\":[]} # Audit example (needs a file upload).\n",
        "    ]\n",
        "    # theme=\"soft\" # We could change the look here if we wanted.\n",
        ")\n",
        "\n",
        "# This makes sure the app only launches when we run this file directly.\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"‚úÖ System Ready! Click the public link below to test.\") # Yay, it's ready!\n",
        "    demo.launch(debug=True, share=True) # Starts the Gradio app!\n",
        "    # 'debug=True' helps with troubleshooting.\n",
        "    # 'share=True' creates a temporary public link to share the app."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPE9MkTrmaxwJYbnOH1cSyp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}