{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olorunfemibabalola/Bias-Detection-NLP/blob/main/MLA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UZgs8opTDQoI"
      },
      "outputs": [],
      "source": [
        "# PROJECT: Explainable Depression Risk and Intent Classifier for Tweets\n",
        "# UNIT: Machine Learning and Applications (Level 7)\n",
        "# AUTHOR: Olorunfemi\n",
        "# DESCRIPTION: A Multi-Output Machine Learning system utilizing Multimodal\n",
        "# Feature Fusion and SHAP explainability to detect depression risk and intent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6sz8_PvDto6"
      },
      "source": [
        "LIBRARY SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KW_H0XdJDyyD",
        "outputId": "0c691b6f-49b6-4e1b-d233-477cbf8c8245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shap in /usr/local/lib/python3.12/dist-packages (0.50.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from shap) (1.16.3)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.12/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.12/dist-packages (from shap) (25.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.12/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from shap) (4.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2026.1.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install shap nltk pandas numpy scikit-learn matplotlib seaborn requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IQ4Mj8xxD3eN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, recall_score, f1_score, accuracy_score, precision_score, confusion_matrix, ConfusionMatrixDisplay\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRVMS0uyD6nx",
        "outputId": "63abfd19-5210-499a-d011-2f54ee242e7b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Download necessary NLTK lexicons (Quietly)\n",
        "nltk.download('vader_lexicon', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3-cK65FwEClT"
      },
      "outputs": [],
      "source": [
        "# Configuration for Reproducibility\n",
        "RANDOM_STATE = 42\n",
        "MAX_FEATURES = 1000  # For TF-IDF to keep training fast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koK56th2EFpP"
      },
      "source": [
        "MODULE 1: DATA ACQUISITION & SYNTHESIS (The \"Safety Net\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "I9dhyqFTEWe-"
      },
      "outputs": [],
      "source": [
        "def generate_synthetic_data(n_samples=500):\n",
        "    \"\"\"\n",
        "    Generates synthetic data if real data is missing. It's like a backup dataset!\n",
        "    Ensures the pipeline runs immediately for reporting/video.\n",
        "    \"\"\"\n",
        "    print(\"\\n‚ö†Ô∏è  DATA STATUS: Real CSV not found. Generating SYNTHETIC DATA for demonstration...\")\n",
        "    print(\"    (This allows you to generate report graphs immediately.)\")\n",
        "\n",
        "    data = {'text': [], 'risk_label': [], 'intent': []}\n",
        "\n",
        "    # Intent: Cry for Help (High Risk)\n",
        "    helps = [\n",
        "        \"I can't do this anymore, please someone help.\", \"Planning to end it tonight.\",\n",
        "        \"There is no hope left, I want the pain to stop.\", \"I feel like I'm drowning.\",\n",
        "        \"Goodbye everyone, I can't take this life anymore.\"\n",
        "    ]\n",
        "    # Intent: Venting (Medium/High Risk)\n",
        "    vents = [\n",
        "        \"So frustrated with everything right now.\", \"Why is life so exhausting?\",\n",
        "        \"Crying in my room again, standard Tuesday.\", \"My anxiety is through the roof.\",\n",
        "        \"I hate how I feel, just needed to say it.\"\n",
        "    ]\n",
        "    # Intent: Loneliness (Medium Risk)\n",
        "    lonely = [\n",
        "        \"I haven't spoken to a human in days.\", \"Does anyone else feel invisible?\",\n",
        "        \"I miss having friends who care.\", \"The silence is too loud.\",\n",
        "        \"Just sitting here alone again.\"\n",
        "    ]\n",
        "    # Intent: Neutral (Low Risk)\n",
        "    neutral = [\n",
        "        \"Watching Netflix and chilling.\", \"Great workout today!\",\n",
        "        \"Anyone see the game last night?\", \"Coffee is life.\",\n",
        "        \"Just finished my homework.\"\n",
        "    ]\n",
        "\n",
        "    for _ in range(n_samples // 4):\n",
        "        data['text'].append(np.random.choice(helps))\n",
        "        data['risk_label'].append(1)\n",
        "        data['intent'].append(\"Cry for Help\")\n",
        "\n",
        "        data['text'].append(np.random.choice(vents))\n",
        "        data['risk_label'].append(1)\n",
        "        data['intent'].append(\"Venting\")\n",
        "\n",
        "        data['text'].append(np.random.choice(lonely))\n",
        "        data['risk_label'].append(1)\n",
        "        data['intent'].append(\"Loneliness\")\n",
        "\n",
        "        data['text'].append(np.random.choice(neutral))\n",
        "        data['risk_label'].append(0)\n",
        "        data['intent'].append(\"Neutral\")\n",
        "\n",
        "    return pd.DataFrame(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "s85Y-MNyGxcI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9_a2AuPyEXin"
      },
      "outputs": [],
      "source": [
        "def load_and_prep_data():\n",
        "    \"\"\"\n",
        "    Tries to load our main dataset ('Suicide_Detection.csv').\n",
        "    If it can't find it (like, if we forgot to upload it!), it falls back to making fake data using `generate_synthetic_data`.\n",
        "    \"\"\"\n",
        "    # Check for Raw Kaggle file\n",
        "    try:\n",
        "        df_raw = pd.read_csv('Suicide_Detection.csv', on_bad_lines='skip', engine='python')\n",
        "        print(\"‚úÖ DATA STATUS: Loaded raw 'Suicide_Detection.csv'. processing sample...\")\n",
        "\n",
        "        # Sample & Label Logic (Distinction Novelty)\n",
        "        df_high = df_raw[df_raw['class'] == 'suicide'].sample(5000, random_state=RANDOM_STATE)\n",
        "        df_low = df_raw[df_raw['class'] == 'non-suicide'].sample(4500, random_state=RANDOM_STATE)\n",
        "        df = pd.concat([df_high, df_low]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "        # Map Risk\n",
        "        df['risk_label'] = df['class'].map({'suicide': 1, 'non-suicide': 0})\n",
        "\n",
        "        # Pseudo-Label Intent (Rule-Based Novelty)\n",
        "        def get_intent(text):\n",
        "            t = str(text).lower()\n",
        "            if any(x in t for x in ['help', 'end', 'die', 'suicide', 'goodbye', 'kill', 'death', 'can\\'t go on with life', 'over it', 'wish i wasn\\'t here', 'i give up', 'no more', 'escape', 'fade away', 'take me away', 'done with life']): return \"Cry for Help\"\n",
        "            if any(x in t for x in ['alone', 'lonely', 'nobody', 'friend', 'no one', 'no friend', 'isolated', 'by myself', 'solitude', 'deserted', 'forgotten', 'empty house', 'on my own', 'single', 'unaccompanied']): return \"Loneliness\"\n",
        "            if any(x in t for x in ['tired', 'sad', 'cry', 'pain', 'hate', 'not good enough', 'it sucks', 'losing myself', 'losing', 'frustrated', 'stressed', 'annoyed', 'upset', 'angry', 'suffering', 'miserable', 'depressed', 'unhappy', 'agony']): return \"Venting\"\n",
        "            if any(x in t for x in ['good', 'happy', 'great', 'fun', 'enjoy', 'positive', 'chill', 'relax', 'weekend', 'food', 'movie', 'book', 'game', 'learn', 'work', 'study', 'exercise', 'music', 'art', 'news', 'weather', 'sleep', 'rest']): return \"Neutral\"\n",
        "            return \"Neutral\"\n",
        "\n",
        "        df['intent'] = df.apply(lambda x: get_intent(x['text']) if x['risk_label'] == 1 else \"Neutral\", axis=1)\n",
        "\n",
        "        return df[['text', 'risk_label', 'intent']]\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        pass\n",
        "\n",
        "    # 3. Fallback\n",
        "    return generate_synthetic_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UmJzWiz1aNYp"
      },
      "outputs": [],
      "source": [
        "#load_and_prep_data().value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icYUXy2IElAG"
      },
      "source": [
        "MODULE 2: PREPROCESSING & FEATURE ENGINEERING (Multimodal Fusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pwawnvmjEpkl"
      },
      "outputs": [],
      "source": [
        "def clean_tweet(text):\n",
        "    \"\"\"\n",
        "    This function cleans up the tweet text by removing stuff we don't need,\n",
        "    like links, mentions, hashtags, and punctuation. Super important for good analysis!\n",
        "    \"\"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'http\\S+', '', text)    # Remove URLs\n",
        "    text = re.sub(r'@\\w+', '', text)       # Remove Mentions\n",
        "    text = re.sub(r'#', '', text)          # Remove Hashtag symbol\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)    # Remove punctuation\n",
        "    return text.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-a2TpTadEwL8"
      },
      "outputs": [],
      "source": [
        "def multimodal_fusion(df):\n",
        "    \"\"\"\n",
        "    This is where we get fancy! We turn the text into numbers by combining different types of features:\n",
        "    1. The length of the tweet.\n",
        "    2. How positive or negative the tweet sounds (sentiment).\n",
        "    3. Important keywords using TF-IDF (Term Frequency-Inverse Document Frequency).\n",
        "    \"\"\"\n",
        "    print(\"\\nüîÑ EXECUTION: Running Multimodal Feature Fusion...\")\n",
        "\n",
        "    # 1. Structural Mode (Tweet Length)\n",
        "    df['text_len'] = df['text'].apply(len)\n",
        "\n",
        "    # 2. Psychological Mode (VADER Sentiment)\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "    df['compound_sent'] = df['text'].apply(lambda x: sid.polarity_scores(str(x))['compound'])\n",
        "\n",
        "    # 3. Semantic Mode (TF-IDF)\n",
        "    tfidf = TfidfVectorizer(max_features=MAX_FEATURES, stop_words='english')\n",
        "    tfidf_matrix = tfidf.fit_transform(df['cleaned_text'])\n",
        "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
        "\n",
        "    # FUSION STEP: Concatenate all features\n",
        "    X = pd.concat([\n",
        "        tfidf_df.reset_index(drop=True),\n",
        "        df[['compound_sent', 'text_len']].reset_index(drop=True)\n",
        "    ], axis=1)\n",
        "\n",
        "    print(f\"   Shape of Fused Feature Matrix: {X.shape}\")\n",
        "    return X, tfidf, sid\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K9mAwcBExK7"
      },
      "source": [
        "MODULE 3: PRESCRIPTIVE ENGINE (Wellness Suggestions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mESNvE-xE4-9"
      },
      "outputs": [],
      "source": [
        "def get_wellness_suggestion(intent_label):\n",
        "    \"\"\"\n",
        "    After figuring out what someone's tweet means (their 'intent'),\n",
        "    this function gives a helpful suggestion based on that intent.\n",
        "    \"\"\"\n",
        "    strategies = {\n",
        "        \"Cry for Help\": \"üî¥ URGENT: High distress detected. Please contact a helpline immediately. Grounding: Name 5 things you can see.\",\n",
        "        \"Venting\": \"üîµ Insight: Emotional release is healthy. Suggestion: Take time off or try expressive journaling to process these feelings.\",\n",
        "        \"Loneliness\": \"üü° Insight: Isolation detected. Suggestion: Reach out to a trusted friend or join an online community.\",\n",
        "        \"Neutral\": \"üü¢ Status: No risk detected. Suggestion: Maintain current self-care routine.\",\n",
        "        \"Unknown\": \"‚ö™ Suggestion: Monitor mood.\"\n",
        "    }\n",
        "    return strategies.get(intent_label, strategies['Unknown'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns6p4NjHE6Mu"
      },
      "source": [
        "MODULE 4: URL HANDLING (Twitter Scraper Fallback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Lb_A3klSFA4P"
      },
      "outputs": [],
      "source": [
        "def get_tweet_from_url(url):\n",
        "    \"\"\"\n",
        "    If someone gives us a Twitter URL, this function tries to grab the actual tweet text from it.\n",
        "    It's a neat trick for getting data from web pages!\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîó URL Detected: {url}\")\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (compatible; Discordbot/2.0; +https://discordapp.com)'}\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            meta = soup.find('meta', property='og:description')\n",
        "            if meta: return meta['content'].strip('‚Äú').strip('‚Äù')\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Graceful Fallback for Video Demo\n",
        "    print(\"üîí X.com API restrictions active. Please input text manually for demo:\")\n",
        "    return input(\"   Paste Tweet Text: \")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox8kK2evFLEp"
      },
      "source": [
        "MODEL EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qL9NeAy9Dnb4"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(Y_test, lr_pred, rf_pred, rf, X_test, tfidf_vectorizer, sid, le):\n",
        "    \"\"\"\n",
        "    Evaluates model performance, generates plots, performs SHAP explainability,\n",
        "    and conducts an ethical fairness check.\n",
        "    \"\"\"\n",
        "    # 5. Metrics Calculation & Plotting: How well did they do?\n",
        "    # Helper function to get recall, precision, and F1 for the risk prediction\n",
        "    def get_metrics(y_true, y_pred):\n",
        "        risk_true = y_true.iloc[:,0] # Actual risk labels\n",
        "        risk_pred = y_pred[:,0] # Predicted risk labels\n",
        "        recall = recall_score(risk_true, risk_pred)\n",
        "        precision = precision_score(risk_true, risk_pred, zero_division=0)\n",
        "        f1 = f1_score(risk_true, risk_pred, average='weighted') # Weighted F1 for imbalanced data\n",
        "        return recall, precision, f1\n",
        "\n",
        "    # Calculate metrics for both models\n",
        "    lr_recall, lr_precision, lr_f1 = get_metrics(Y_test, lr_pred)\n",
        "    rf_recall, rf_precision, rf_f1 = get_metrics(Y_test, rf_pred)\n",
        "\n",
        "    # Print out a neat table of results\n",
        "    print(f\"\\nüìä RESULTS TABLE (Use in Report):\")\n",
        "    print(f\"   Model                |    Risk Recall  |   Risk Precision   | F1-Score (Weighted)\")\n",
        "    print(f\"   ---------------------|-----------------|--------------------|--------------------\")\n",
        "    print(f\"   Logistic Regression  | {lr_recall:.2%} | {lr_precision:.2%} | {lr_f1:.2%}\")\n",
        "    print(f\"   Random Forest        | {rf_recall:.2%} | {rf_precision:.2%} | {rf_f1:.2%}\")\n",
        "\n",
        "    # Plot a comparison of the metrics\n",
        "    plt.figure(figsize=(10,6))\n",
        "    bar_width = 0.2\n",
        "    index = np.arange(2)\n",
        "\n",
        "    plt.bar(index, [lr_recall, rf_recall], bar_width, label='Recall', color='teal')\n",
        "    plt.bar(index + bar_width, [lr_precision, rf_precision], bar_width, label='Precision', color='orange')\n",
        "    plt.bar(index + 2*bar_width, [lr_f1, rf_f1], bar_width, label='F1-Score', color='purple')\n",
        "\n",
        "    plt.xlabel('Model')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Model Performance Comparison (Risk Label)')\n",
        "    plt.xticks(index + bar_width, ['Logistic Regression', 'Random Forest'])\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"comparison_metrics_plot.png\") # Save the plot\n",
        "    print(\"   -> Saved 'comparison_metrics_plot.png'\")\n",
        "\n",
        "    # Confusion Matrix for Random Forest (Risk Label): Visualizing correct/incorrect predictions\n",
        "    print(\"\\nüìâ CONFUSION MATRIX: Generating for Random Forest (Risk Label)...\")\n",
        "    cm = confusion_matrix(Y_test.iloc[:,0], rf_pred[:,0]) # Calculate confusion matrix\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Low Risk', 'High Risk']) # Prepare for display\n",
        "    plt.figure(figsize=(6,6))\n",
        "    disp.plot(cmap=plt.cm.Blues, values_format='d') # Plot it!\n",
        "    plt.title('Confusion Matrix: Random Forest (Risk Label)')\n",
        "    plt.savefig(\"confusion_matrix_rf.png\") # Save the confusion matrix plot\n",
        "    print(\"   -> Saved 'confusion_matrix_rf.png'\")\n",
        "\n",
        "    print(\"\\nüìâ CONFUSION MATRIX: Generating for Logistic Regression (Risk Label)...\")\n",
        "    cm = confusion_matrix(Y_test.iloc[:,0], lr_pred[:,0]) # Calculate confusion matrix\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Low Risk', 'High Risk']) # Prepare for display\n",
        "    plt.figure(figsize=(6,6))\n",
        "    disp.plot(cmap=plt.cm.Blues, values_format='d') # Plot it!\n",
        "    plt.title('Confusion Matrix: Random Forest (Risk Label)')\n",
        "    plt.savefig(\"confusion_matrix_rf.png\") # Save the confusion matrix plot\n",
        "    print(\"   -> Saved 'confusion_matrix_rf.png'\")\n",
        "\n",
        "    # 6. SHAP Explainability: Why did the model make that decision?\n",
        "    print(\"\\nüîç EXPLAINABILITY: Generating SHAP Summary Plot...\")\n",
        "    explainer = shap.TreeExplainer(rf.estimators_[0]) # SHAP for the Random Forest's first estimator (the risk model)\n",
        "\n",
        "    # Sample a small part of the test set for faster SHAP calculation\n",
        "    shap_sample = X_test.iloc[:50]\n",
        "    # Get SHAP values, focusing on the positive class (risk_label=1)\n",
        "    shap_values = explainer.shap_values(shap_sample, check_additivity=False)\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    try:\n",
        "        if isinstance(shap_values, list) and len(shap_values) > 1:\n",
        "            # If SHAP returns values for both classes, pick the positive one\n",
        "            shap.summary_plot(shap_values[1], shap_sample, show=False)\n",
        "        else:\n",
        "            # Otherwise, use the direct SHAP values\n",
        "            shap.summary_plot(shap_values, shap_sample, show=False)\n",
        "\n",
        "        plt.title(\"SHAP: Feature Impact on Depression Risk\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"shap_summary.png\") # Save the SHAP plot\n",
        "        print(\"   -> Saved 'shap_summary.png' \")\n",
        "    except Exception as e:\n",
        "        print(f\"   -> SHAP summary plot could not be generated. Error: {e}\")\n",
        "\n",
        "    # 7. Ethical Fairness Check: Is our model fair across different types of tweets?\n",
        "    print(\"\\n‚öñÔ∏è ETHICS CHECK: Analyzing Disparate Recall...\")\n",
        "    test_df = X_test.copy() # Make a copy for analysis\n",
        "    test_df['pred'] = rf_pred[:, 0] # Add predictions\n",
        "    test_df['true'] = Y_test.iloc[:, 0] # Add actual labels\n",
        "\n",
        "    # Separate tweets by length to check for bias\n",
        "    short = test_df[test_df['text_len'] < 50]\n",
        "    long_t = test_df[test_df['text_len'] >= 50]\n",
        "\n",
        "    # Calculate recall for short vs. long tweets\n",
        "    rec_s = recall_score(short['true'], short['pred']) if len(short) > 0 else 0\n",
        "    rec_l = recall_score(long_t['true'], long_t['pred']) if len(long_t) > 0 else 0\n",
        "\n",
        "    print(f\"   Recall on Short Tweets: {rec_s:.2%}\")\n",
        "    print(f\"   Recall on Long Tweets:  {rec_l:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAIN PIPELINE"
      ],
      "metadata": {
        "id": "0hNatZJKRf77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pipeline():\n",
        "    # 1. Data Prep: Get our tweet data ready\n",
        "    df = load_and_prep_data() # Load and clean up raw tweet data\n",
        "    df['cleaned_text'] = df['text'].apply(clean_tweet) # Apply our custom cleaning function\n",
        "\n",
        "    # 2. Features: Turn text into numbers our model can understand\n",
        "    X, tfidf_vectorizer, sid = multimodal_fusion(df) # Combine length, sentiment (VADER), and keyword (TF-IDF) features\n",
        "\n",
        "    # 3. Targets (Multi-Output Setup): What are we trying to predict?\n",
        "    le = LabelEncoder() # For converting intent names to numbers\n",
        "    df['intent_enc'] = le.fit_transform(df['intent']) # Encode the 'intent' column\n",
        "    Y = df[['risk_label', 'intent_enc']] # Our two targets: risk (0/1) and intent (encoded number)\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=RANDOM_STATE)\n",
        "\n",
        "    # 4. Model Comparison: Let's see which model does better!\n",
        "    print(\"\\nüß† TRAINING: Comparing Logistic Regression vs Random Forest Model...\")\n",
        "\n",
        "    # Baseline: Logistic Regression (a simpler model for comparison)\n",
        "    lr = MultiOutputClassifier(LogisticRegression(max_iter=1000)) # Handles multiple outputs (risk & intent)\n",
        "    lr.fit(X_train, Y_train)\n",
        "    lr_pred = lr.predict(X_test)\n",
        "\n",
        "    # Distinction: Random Forest (our main, more complex model)\n",
        "    rf_base = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=RANDOM_STATE) # 'balanced' helps with uneven classes\n",
        "    rf = MultiOutputClassifier(rf_base)\n",
        "    rf.fit(X_train, Y_train)\n",
        "    rf_pred = rf.predict(X_test)\n",
        "\n",
        "    # Call the new evaluation function\n",
        "    evaluate_model(Y_test, lr_pred, rf_pred, rf, X_test, tfidf_vectorizer, sid, le)\n",
        "\n",
        "    # 8. Try it out yourself!\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"üöÄ WE'RE LIVE!!!\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    while True: # Loop to allow multiple inputs\n",
        "        user_in = input(\"Enter Tweet Text OR Twitter URL (type 'quit' to exit): \").strip()\n",
        "\n",
        "        if user_in.lower() == 'quit': # Exit condition\n",
        "            print(\"Exiting live demo. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if user_in.startswith(\"http\"): # If it's a URL, try to scrape the tweet text\n",
        "            text_in = get_tweet_from_url(user_in)\n",
        "        else:\n",
        "            text_in = user_in # Otherwise, it's direct text input\n",
        "\n",
        "        # Process Input for the model\n",
        "        clean_in = clean_tweet(text_in) # Clean the user's input\n",
        "        feat_tfidf = tfidf_vectorizer.transform([clean_in]).toarray() # Convert to TF-IDF features\n",
        "        feat_sent = sid.polarity_scores(clean_in)['compound'] # Get sentiment score\n",
        "        feat_len = len(clean_in) # Get text length\n",
        "\n",
        "        # Combine all features into one array\n",
        "        feat_full = np.hstack([feat_tfidf, [[feat_sent, feat_len]]])\n",
        "\n",
        "        # Predict risk and intent using the Logistic Regression model (can be changed to RF if preferred)\n",
        "        pred_raw = lr.predict(feat_full)\n",
        "        try:\n",
        "            intent_res = le.inverse_transform([pred_raw[0][1]])[0] # Decode intent label\n",
        "        except:\n",
        "            intent_res = \"Unknown\" # Fallback if decoding fails\n",
        "\n",
        "        # Derive granular risk level based on predicted intent\n",
        "        if intent_res == \"Cry for Help\":\n",
        "            risk_res = \"High Risk\"\n",
        "        elif intent_res == \"Venting\":\n",
        "            risk_res = \"Moderate Risk\"\n",
        "        elif intent_res == \"Loneliness\":\n",
        "            risk_res = \"Low Risk\"\n",
        "        elif intent_res == \"Neutral\":\n",
        "            risk_res = \"No Risk\"\n",
        "        else:\n",
        "            risk_res = \"Uncertain Risk\" # For 'Unknown' or other unexpected intents\n",
        "\n",
        "        # Display the analysis and a wellness suggestion\n",
        "        print(f\"\\nüìù Analysis for: '{text_in}'\")\n",
        "        print(f\"   Risk Level:  {risk_res}\")\n",
        "        print(f\"   Intent:      {intent_res}\")\n",
        "        print(f\"   Suggestion:  {get_wellness_suggestion(intent_res)}\")\n",
        "        print(\"=\"*50)"
      ],
      "metadata": {
        "id": "uMhJjhBMRXgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7SomdnpFQmW",
        "outputId": "31e80dbc-90e3-43f6-d0ad-f976bd8ed5e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ DATA STATUS: Loaded raw 'Suicide_Detection.csv'. processing sample...\n",
            "\n",
            "üîÑ EXECUTION: Running Multimodal Feature Fusion...\n",
            "   Shape of Fused Feature Matrix: (9500, 1002)\n",
            "\n",
            "üß† TRAINING: Comparing Logistic Regression vs Random Forest Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "üöÄ LIVE DEMO SYSTEM (Ready for Video Recording)\n",
            "==================================================\n",
            "Enter Tweet Text OR Twitter URL (type 'quit' to exit): I want to eat so much ice cream today!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìù Analysis for: 'I want to eat so much ice cream today!'\n",
            "   Risk Level:  No Risk\n",
            "   Intent:      Neutral\n",
            "   Suggestion:  üü¢ Status: No risk detected. Suggestion: Maintain current self-care routine.\n",
            "==================================================\n",
            "Enter Tweet Text OR Twitter URL (type 'quit' to exit): quit\n",
            "Exiting live demo. Goodbye!\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    run_pipeline()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMZ2uITZMDa3iR8VnKeXWzY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}