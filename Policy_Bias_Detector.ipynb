{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTzxjWHv7l/Rk4B043v0lR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olorunfemibabalola/Bias-Detection-NLP/blob/main/Policy_Bias_Detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Import modules\n",
        "Import the dataset\n",
        "Perform exploratory data analysis\n",
        "Clean the data\n",
        "Split into training and testing sets\n",
        "Create a model\n",
        "Train the model\n",
        "Make predictions\n",
        "Test the model\n",
        "Evaluate the model\n",
        "Make predictions on new data\n",
        "Persist the model for future use\n",
        "Load a persisted model\n",
        "Make predictions on new data\n",
        "'''"
      ],
      "metadata": {
        "id": "TdVTNgfPevCf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "f7982bd5-5f99-4014-d1f1-e002262c343b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nImport modules\\nImport the dataset\\nPerform exploratory data analysis\\nClean the data\\nSplit into training and testing sets\\nCreate a model\\nTrain the model\\nMake predictions\\nTest the model\\nEvaluate the model\\nMake predictions on new data\\nPersist the model for future use\\nLoad a persisted model\\nMake predictions on new data\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------------------------------\n",
        "\n"
      ],
      "metadata": {
        "id": "QuT3Jr3ugEAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers torch PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRQ3OUHTFmp3",
        "outputId": "bee62396-0da5-40ed-cd5f-5cf6e254dc96"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import fileinput\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "hA28fP8KgFS3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "userChoice = input(\"Woud you like to input a text or a file? (text/file)\").lower().strip()\n",
        "while (userChoice != \"text\") and (userChoice != \"file\"):\n",
        "    print(\"Invalid input. Please enter 'text' or 'file'\")\n",
        "    userChoice = input(\"Woud you like to input a text or a file? (text/file)\").lower().strip()\n",
        "else:\n",
        "    print(f\"You have selected {userChoice}\")\n",
        "\n",
        "if userChoice == \"text\":\n",
        "  userText = input(\"Enter your text:\")\n",
        "else:\n",
        "  print(\"Upload your file below:\")\n",
        "  #This creates the \"Upload\" button\n",
        "  uploadedFile = files.upload()\n",
        "\n",
        "  # The 'uploaded' variable is a dictionary:\n",
        "  # Key = filename, Value = file data (bytes)\n",
        "  for filename in uploadedFile.keys():\n",
        "    print(f'User uploaded file \"{filename}\" with a length of {len(uploadedFile[filename])} bytes')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3xd4lIL8Xgtu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26f8c3dd-8bfe-4e49-d473-0a5c9d2ce898"
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Woud you like to input a text or a file? (text/file)text\n",
            "You have selected text\n",
            "Enter your text:hvv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "215b614e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "dd7c8ff8-e187-4e4d-a0d5-21fae1c7e94a"
      },
      "source": [
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# Define the PDF file name\n",
        "# The filename variable already holds the correct name from previous steps\n",
        "\n",
        "# Open the PDF file in binary read mode\n",
        "reader = PdfReader(filename)\n",
        "\n",
        "# Initialize an empty string to store the extracted text\n",
        "user_file= \"\"\n",
        "\n",
        "# Iterate through each page and extract text\n",
        "for page in reader.pages:\n",
        "    user_file += page.extract_text()\n",
        "\n",
        "# Display a portion of the extracted text to verify\n",
        "print(user_file[:1000])\n",
        "print(f\"Total characters extracted: {len(user_file)}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'filename' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2717086768.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Open the PDF file in binary read mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPdfReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Initialize an empty string to store the extracted text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'filename' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "227c1018",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "86a90219-dcfd-4cda-cbe1-4942e82a35e9"
      },
      "source": [
        "print(user_file)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'user_file' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1095773093.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'user_file' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_file.lower()\n",
        "new = user_file.split(\".\")"
      ],
      "metadata": {
        "id": "OAe9PvUCVvR8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "1d9bb470-fb7a-45f9-a81c-b5e800e6379c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'user_file' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1643697784.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0muser_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'user_file' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q transformers torch\n",
        "\n",
        "import torch\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Step 1: Initialize the Auditor (RoBERTa Encoder)\n",
        "# Specialized for detecting social and cognitive biases\n",
        "auditor = pipeline(\"text-classification\",\n",
        "                   model=\"valurank/distilroberta-bias\",\n",
        "                   device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "# Step 2: Initialize the Conversationalist (GPT-2 Decoder)\n",
        "chat_model_name = \"openai-community/gpt2\"\n",
        "chat_tokenizer = AutoTokenizer.from_pretrained(chat_model_name)\n",
        "chat_model = AutoModelForCausalLM.from_pretrained(chat_model_name)\n",
        "\n",
        "def get_chatbot_response(prompt):\n",
        "    # Perform Bias Audit FIRST\n",
        "    audit_result = auditor(prompt)[0]\n",
        "\n",
        "    # If Bias is detected above a 70% confidence threshold\n",
        "    if audit_result['label'] == 'Biased' and audit_result['score'] > 0.7:\n",
        "        return f\"‚ö†Ô∏è [BIAS ALERT]: I detected potential {audit_result['label']} in your prompt. \" \\\n",
        "               f\"Please rephrase to be more inclusive.\"\n",
        "\n",
        "    # Otherwise, generate a standard response\n",
        "    inputs = chat_tokenizer.encode(prompt + chat_tokenizer.eos_token, return_tensors='pt')\n",
        "    outputs = chat_model.generate(inputs, max_length=100, do_sample=True, top_k=50, top_p=0.95)\n",
        "    return chat_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Step 3: Interactive Chat Loop\n",
        "print(\"üõ°Ô∏è Inclusive Assistant Active. Type 'quit' to stop.\")\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in ['quit', 'exit']: break\n",
        "\n",
        "    response = get_chatbot_response(user_input)\n",
        "    print(f\"Bot: {response}\\n\")\n"
      ],
      "metadata": {
        "id": "3uhTroqYdedj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "599788a3-1bbb-4113-b1c4-64c6fdae6016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üõ°Ô∏è Inclusive Assistant Active. Type 'quit' to stop.\n",
            "You: hey\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: heyYou may have seen a great movie or heard from a great actor and a great story you can enjoy at any time. But how do you get to it? You just need to know how.\n",
            "\n",
            "A couple of things you need to know about AIM Aims:\n",
            "\n",
            "AIM Aims is a toolkit on their site to help you take a hard look at what to watch and don't watch at certain times of year. I've used it for my work to see\n",
            "\n",
            "You: i hate women\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure you have the necessary libraries installed\n",
        "# !pip install -q PyPDF2 gradio transformers torch\n",
        "\n",
        "import PyPDF2\n",
        "import gradio as gr\n",
        "\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    \"\"\"Extracts text from an uploaded PDF file object.\"\"\"\n",
        "    reader = PyPDF2.PdfReader(pdf_file.name)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        content = page.extract_text()\n",
        "        if content:\n",
        "            text += content + \" \"\n",
        "    return text\n",
        "\n",
        "def audit_document(file):\n",
        "    \"\"\"Scans the entire document and returns biased sentences.\"\"\"\n",
        "    if file is None:\n",
        "        return \"No file uploaded.\"\n",
        "\n",
        "    text = extract_text_from_pdf(file)\n",
        "    sentences = text.split('.')\n",
        "    biased_findings = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        clean_sentence = sentence.strip()\n",
        "        if len(clean_sentence) > 10: # Filter out noise\n",
        "            result = auditor(clean_sentence)[0]\n",
        "            if result['label'] == 'Biased' and result['score'] > 0.75:\n",
        "                biased_findings.append(f\"‚Ä¢ \\\"{clean_sentence}\\\" (Confidence: {result['score']:.2f})\")\n",
        "\n",
        "    if not biased_findings:\n",
        "        return \"‚úÖ No biased statements found. The document is safe and inclusive.\"\n",
        "    else:\n",
        "        report = \"‚ö†Ô∏è Potential bias detected in the following statements:\\n\\n\" + \"\\n\".join(biased_findings)\n",
        "        return report\n",
        "\n",
        "# Updated Chatbot Logic with \"Document Upload\" Intent\n",
        "def chatbot_response(message, history):\n",
        "    message_lower = message.lower()\n",
        "\n",
        "    # Feature 1: Check for Document Intent\n",
        "    if any(word in message_lower for word in [\"upload\", \"document\", \"file\", \"scan\"]):\n",
        "        return \"Sure! Please use the **Upload File** button below to provide your document for a bias audit.\"\n",
        "\n",
        "    # Feature 2: Standard Chat Audit\n",
        "    res = auditor(message)[0]\n",
        "    if res['label'] == 'Biased':\n",
        "        return f\"‚ö†Ô∏è BIAS ALERT: I detected potential {res['label']} in your input. Try using neutral phrasing.\"\n",
        "\n",
        "    return \"‚úÖ Inclusive input. I am ready to help with your policy questions.\"\n",
        "\n",
        "# UI Layout (Using Gradio Blocks for better control)\n",
        "with gr.Blocks(theme=\"soft\") as demo:\n",
        "    gr.Markdown(\"# üõ°Ô∏è Inclusive Policy Assistant & Auditor\")\n",
        "\n",
        "    with gr.Tab(\"Chat\"):\n",
        "        gr.ChatInterface(chatbot_response)\n",
        "\n",
        "    with gr.Tab(\"Document Audit\"):\n",
        "        file_input = gr.File(label=\"Upload Corporate Policy (PDF)\")\n",
        "        audit_output = gr.Textbox(label=\"Audit Report\", lines=10)\n",
        "        audit_button = gr.Button(\"Run Audit\")\n",
        "        audit_button.click(audit_document, inputs=file_input, outputs=audit_output)\n",
        "\n",
        "demo.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757
        },
        "id": "K2qYD1YWaCi9",
        "outputId": "9d220fdb-e9c1-4c3e-86f6-68678b3d1a82"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2350190090.py:55: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=\"soft\") as demo:\n",
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://6186570618f502c488.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6186570618f502c488.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://6186570618f502c488.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}